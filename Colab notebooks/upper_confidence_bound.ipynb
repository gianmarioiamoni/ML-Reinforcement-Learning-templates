{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"12SoNSOpj7cnoKxvW6n64VN9ce0LBDv1f","timestamp":1729866430152}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QJdrJJQUv3-k"},"source":["# Upper Confidence Bound (UCB)"]},{"cell_type":"markdown","metadata":{"id":"2XCjepjJwEv-"},"source":["## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"l_mBkG3YwNTt","executionInfo":{"status":"ok","timestamp":1729871585326,"user_tz":-120,"elapsed":579,"user":{"displayName":"Gianmario Iamoni","userId":"07995532745716785055"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"npqlXjaNwYTv"},"source":["## Importing the dataset"]},{"cell_type":"code","metadata":{"id":"HMJfUVLVwcFc","executionInfo":{"status":"ok","timestamp":1729871586753,"user_tz":-120,"elapsed":305,"user":{"displayName":"Gianmario Iamoni","userId":"07995532745716785055"}}},"source":["# Importing the dataset\n","#\n","# The data set if a simulation of a real time activity where\n","# to different users is shown randomly an adv choosen in a set of 10\n","# and we record with 0/1 if the user wouldn't or would click on the showed AD\n","#\n","# The 0 and 1 in the dataset are in fact the rewards\n","#\n","# For reinforcement learning we don't have to create a matrix of features\n","# or dependant variables\n","dataset = pd.read_csv('Ads_CTR_Optimisation.csv')"],"execution_count":13,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Rw8VdJu_YjBG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PaSbots_wfoB"},"source":["## Implementing UCB"]},{"cell_type":"code","source":["# Implementing the UCB\n","#\n","# 1. at each round (user) n, we consider 2 numbers for each ADi:\n","#   - Ni(n) = the number of times the ADi was selected up to round n\n","#   - Ri(n) = the sum of rewards of the ADVi up to round n\n","import math\n","N = 10000 # total number of users (rounds)\n","d = 10 # number of ADs\n","ads_selected = [] # list of selected ADs at each round\n","numbers_of_selections = [0] * d # Ni(n) - initialised as a list of ten 0\n","sums_of_rewards = [0] * d # Ri(n)\n","total_reward = 0 # the sum of all the rewards received over the round\n","\n","# 2. From these 2 numbers we compute:\n","#   - the average reward of ADi up to round n:\n","#     r''i(n) = Ri(n)/Ni(n)\n","#\n","#   - the confidence interval [r''i(n) - DELTAi(n), r''i(n)+DELTAi(n)],where:\n","#     DELTAi(n) = SQRT(3log(n)/2Ni(n))\n","#\n","# 3. We select the ADi that has the maximum UCB r''(n)+DELTAi(n)\n","for n in range(0, N):\n","  # per each round:\n","  # we select an AD, starting from the first\n","  ad = 0\n","  max_upper_bound = 0 # store the max UCB of each round\n","  # loop over the different ADs, to compare the maximum Upper Confidence Bound\n","  # by comparing the UCB of each of the ADs\n","  for i in range(0, d):\n","    # per each AD\n","    if numbers_of_selections[i] > 0:\n","      # if the AD has been selected at least once\n","      average_reward = sums_of_rewards[i] / numbers_of_selections[i]\n","      # n start from 0 in the loop, so we use n+1 in the log\n","      delta_i = math.sqrt(3/2 * math.log(n + 1) / numbers_of_selections[i])\n","      upper_bound = average_reward + delta_i # maximum UCB\n","    else:\n","      # if the AD has not been selected at least once\n","      # we have to select it\n","      upper_bound = 1e400 # trick to select all the AD (simulate infinity)\n","\n","    if (upper_bound > max_upper_bound):\n","      # update the max_upper_bound\n","      max_upper_bound = upper_bound\n","      # select the AD with max uper bound\n","      ad = i\n","\n","    # update the variables\n","    ads_selected.append(ad) # add the selected AD to the list of selected ADs\n","    numbers_of_selections[ad] = numbers_of_selections[ad] + 1\n","    reward = dataset.values[n, ad] # get the reward from the dataset\n","    sums_of_rewards[ad] = sums_of_rewards[ad] + reward\n","    total_reward = total_reward + reward # update the total reward at the round n\n","\n","\n","\n","\n"],"metadata":{"id":"YwKUTvByHCA_","executionInfo":{"status":"ok","timestamp":1729872041338,"user_tz":-120,"elapsed":323,"user":{"displayName":"Gianmario Iamoni","userId":"07995532745716785055"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AXftWcjDwsYj"},"source":["## Visualising the results"]},{"cell_type":"code","source":["# Visualising the results\n","#\n","# We plot an histogram which show"],"metadata":{"id":"TmOYjYtTaMIc"},"execution_count":null,"outputs":[]}]}